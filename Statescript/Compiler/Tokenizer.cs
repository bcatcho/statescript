
#line 1 "Tokenizer.rl.cs"
// This file is AUTOGENERATED with RAGEL
// !!DO NOT EDIT!! Change the RL file and compile with Ragel
// http://www.colm.net/open-source/ragel/
// This RL file is the RAGEL file that generates the parser
// Build and test:
// alias mcs=/Library/Frameworks/Mono.framework/Versions/Current/bin/mcs
// ragel -A Parser.rl && mcs *.cs -define:RAGEL_TEST && mono parser.exe
// ragel -o Parser.cs -A ParserDefinition.rl.cs && mcs Parser.cs -define:RAGEL_TEST && mono parser.exe
// make sure AstNode.cs and AstParam.cs are in the same folder
namespace Statescript.Compiler
{
   using System;
   using System.Collections.Generic;

   public enum TokenType
   {
     Keyword,
     Identifier,
     Value,
     TransitionValue,
     MessageValue,
     Operator,
     NewLine
   }

   public enum TokenOperator
   {
     Set,
     Transition
   }

   public struct Token
   {
     public int StartIndex;
     public int Length;
     public int LineNumber;
     public TokenType TokenType;
     public TokenOperator Operator;
   }

   public class Tokenizer
   {
      int _lineNumber = 0;
      bool _tokenUncommitted;
      int _tokenStart { get { return _token.StartIndex; } }
      Token _token;
      private List<Token> _tokens;
      char[] _data;
      // ragel properties
      private int cs;
      int p;

      private void StartToken(TokenType tokenType)
      {
        _token = new Token {
            LineNumber = _lineNumber,
            StartIndex = p,
            TokenType = tokenType
        };
        _tokenUncommitted = true;
      }

      private void StartOperatorToken(TokenOperator tokenOperator)
      {
        _token = new Token {
            LineNumber = _lineNumber,
            StartIndex = p,
            Operator = tokenOperator,
            TokenType = TokenType.Operator,
        };
        _tokenUncommitted = true;
      }

      private void log(string msg) {
        Console.WriteLine(string.Format("{0} {1}", p, msg));
      }

      private void logEnd(string msg) {
        var token = new String(_data, _tokenStart, p - _tokenStart);
        Console.WriteLine(string.Format("{0} {1}: {2}", p, msg, token));
      }

      private void EmitToken() {
        _token.Length = p - _tokenStart;
        _tokens.Add(_token);
        _tokenUncommitted = false;
      }

      private void EmitNewLine() {
        _token.TokenType = TokenType.NewLine;
        _tokens.Add(_token);
        _tokenUncommitted = false;
      }

      private void CommitLastToken() {
        if (_tokenUncommitted) {
          // update length in case the file ended early
          _token.Length = p -_token.StartIndex;
          _tokens.Add(_token);
          _tokenUncommitted = false;
        }
      }

      
#line 108 "tmp/Tokenizer.cs"
static readonly sbyte[] _Tokenizer_actions =  new sbyte [] {
	0, 1, 0, 1, 1, 1, 2, 1, 
	3, 1, 4, 1, 5, 2, 1, 0, 
	2, 1, 2, 2, 1, 3
};

static readonly byte[] _Tokenizer_key_offsets =  new byte [] {
	0, 0, 11, 22, 30, 43, 47, 48, 
	53, 58, 66, 68, 69, 74, 82, 94, 
	99, 108, 114, 125, 132
};

static readonly char[] _Tokenizer_trans_keys =  new char [] {
	'\u000a', '\u000d', '\u0020', '\u0040', '\u005f', '\u0009', '\u000c', '\u0041', 
	'\u005a', '\u0061', '\u007a', '\u000a', '\u000d', '\u0020', '\u0040', '\u005f', 
	'\u0009', '\u000c', '\u0041', '\u005a', '\u0061', '\u007a', '\u0020', '\u005f', 
	'\u0009', '\u000d', '\u0041', '\u005a', '\u0061', '\u007a', '\u000a', '\u000d', 
	'\u0020', '\u002d', '\u005f', '\u0009', '\u000c', '\u0030', '\u0039', '\u0041', 
	'\u005a', '\u0061', '\u007a', '\u0020', '\u002d', '\u0009', '\u000d', '\u003e', 
	'\u0020', '\u0022', '\u0027', '\u0009', '\u000d', '\u005f', '\u0041', '\u005a', 
	'\u0061', '\u007a', '\u0022', '\u005f', '\u0030', '\u0039', '\u0041', '\u005a', 
	'\u0061', '\u007a', '\u000a', '\u000d', '\u000a', '\u005f', '\u0041', '\u005a', 
	'\u0061', '\u007a', '\u0027', '\u005f', '\u0030', '\u0039', '\u0041', '\u005a', 
	'\u0061', '\u007a', '\u000a', '\u000d', '\u0020', '\u002d', '\u0040', '\u005f', 
	'\u0009', '\u000c', '\u0041', '\u005a', '\u0061', '\u007a', '\u000a', '\u0020', 
	'\u002d', '\u0009', '\u000d', '\u000a', '\u0020', '\u005f', '\u0009', '\u000d', 
	'\u0041', '\u005a', '\u0061', '\u007a', '\u000a', '\u000d', '\u0020', '\u0040', 
	'\u0009', '\u000c', '\u000a', '\u000d', '\u0020', '\u0040', '\u005f', '\u0009', 
	'\u000c', '\u0041', '\u005a', '\u0061', '\u007a', '\u000a', '\u000d', '\u0020', 
	'\u002d', '\u0040', '\u0009', '\u000c', '\u000a', '\u000d', '\u0020', '\u002d', 
	'\u0040', '\u005f', '\u0009', '\u000c', '\u0041', '\u005a', '\u0061', '\u007a', 
	(char) 0
};

static readonly sbyte[] _Tokenizer_single_lengths =  new sbyte [] {
	0, 5, 5, 2, 5, 2, 1, 3, 
	1, 2, 2, 1, 1, 2, 6, 3, 
	3, 4, 5, 5, 6
};

static readonly sbyte[] _Tokenizer_range_lengths =  new sbyte [] {
	0, 3, 3, 3, 4, 1, 0, 1, 
	2, 3, 0, 0, 2, 3, 3, 1, 
	3, 1, 3, 1, 3
};

static readonly byte[] _Tokenizer_index_offsets =  new byte [] {
	0, 0, 9, 18, 24, 34, 38, 40, 
	45, 49, 55, 58, 60, 64, 70, 80, 
	85, 92, 98, 107, 114
};

static readonly sbyte[] _Tokenizer_indicies =  new sbyte [] {
	2, 3, 0, 4, 5, 0, 5, 5, 
	1, 7, 8, 6, 9, 10, 6, 10, 
	10, 1, 11, 5, 11, 5, 5, 1, 
	13, 14, 12, 15, 16, 12, 16, 16, 
	16, 1, 17, 18, 17, 1, 19, 1, 
	20, 21, 22, 20, 1, 23, 23, 23, 
	1, 24, 25, 25, 25, 25, 1, 26, 
	27, 1, 28, 1, 29, 29, 29, 1, 
	24, 30, 30, 30, 30, 1, 32, 33, 
	31, 18, 4, 5, 31, 5, 5, 1, 
	34, 17, 18, 17, 1, 35, 11, 5, 
	11, 5, 5, 1, 2, 3, 0, 4, 
	0, 1, 2, 3, 0, 4, 5, 0, 
	5, 5, 1, 32, 33, 31, 18, 4, 
	31, 1, 32, 33, 31, 18, 4, 5, 
	31, 5, 5, 1, 0
};

static readonly sbyte[] _Tokenizer_trans_targs =  new sbyte [] {
	1, 0, 18, 1, 2, 4, 3, 18, 
	16, 2, 2, 3, 5, 19, 15, 6, 
	4, 5, 6, 7, 7, 8, 12, 9, 
	10, 9, 17, 11, 17, 13, 13, 14, 
	20, 14, 19, 18
};

static readonly sbyte[] _Tokenizer_trans_actions =  new sbyte [] {
	0, 0, 1, 1, 7, 9, 3, 13, 
	13, 19, 0, 0, 3, 13, 13, 16, 
	0, 0, 5, 3, 0, 0, 0, 11, 
	3, 0, 1, 1, 0, 11, 0, 0, 
	1, 1, 0, 0
};

const int Tokenizer_start = 17;
const int Tokenizer_first_final = 17;
const int Tokenizer_error = 0;

const int Tokenizer_en_main = 17;


#line 108 "Tokenizer.rl.cs"


      public void Init()
      {
         
#line 209 "tmp/Tokenizer.cs"
	{
	cs = Tokenizer_start;
	}

#line 113 "Tokenizer.rl.cs"
      }

      public List<Token> Tokenize(char[] data, int len)
      {
         if (_tokens == null) {
           _tokens = new List<Token>(128);
         }
         _tokens.Clear();
         _lineNumber = 1; // start at line 1 like most text editors
         _data = data;
         p = 0;
         int pe = len;
         //int eof = len;
         
#line 229 "tmp/Tokenizer.cs"
	{
	sbyte _klen;
	byte _trans;
	int _acts;
	int _nacts;
	byte _keys;

	if ( p == pe )
		goto _test_eof;
	if ( cs == 0 )
		goto _out;
_resume:
	_keys = _Tokenizer_key_offsets[cs];
	_trans = (byte)_Tokenizer_index_offsets[cs];

	_klen = _Tokenizer_single_lengths[cs];
	if ( _klen > 0 ) {
		short _lower = _keys;
		short _mid;
		short _upper = (short) (_keys + _klen - 1);
		while (true) {
			if ( _upper < _lower )
				break;

			_mid = (short) (_lower + ((_upper-_lower) >> 1));
			if ( data[p] < _Tokenizer_trans_keys[_mid] )
				_upper = (short) (_mid - 1);
			else if ( data[p] > _Tokenizer_trans_keys[_mid] )
				_lower = (short) (_mid + 1);
			else {
				_trans += (byte) (_mid - _keys);
				goto _match;
			}
		}
		_keys += (byte) _klen;
		_trans += (byte) _klen;
	}

	_klen = _Tokenizer_range_lengths[cs];
	if ( _klen > 0 ) {
		short _lower = _keys;
		short _mid;
		short _upper = (short) (_keys + (_klen<<1) - 2);
		while (true) {
			if ( _upper < _lower )
				break;

			_mid = (short) (_lower + (((_upper-_lower) >> 1) & ~1));
			if ( data[p] < _Tokenizer_trans_keys[_mid] )
				_upper = (short) (_mid - 2);
			else if ( data[p] > _Tokenizer_trans_keys[_mid+1] )
				_lower = (short) (_mid + 2);
			else {
				_trans += (byte)((_mid - _keys)>>1);
				goto _match;
			}
		}
		_trans += (byte) _klen;
	}

_match:
	_trans = (byte)_Tokenizer_indicies[_trans];
	cs = _Tokenizer_trans_targs[_trans];

	if ( _Tokenizer_trans_actions[_trans] == 0 )
		goto _again;

	_acts = _Tokenizer_trans_actions[_trans];
	_nacts = _Tokenizer_actions[_acts++];
	while ( _nacts-- > 0 )
	{
		switch ( _Tokenizer_actions[_acts++] )
		{
	case 0:
#line 4 "TokenizerDef.rl"
	{ _lineNumber++; log("newline"); EmitNewLine(); }
	break;
	case 1:
#line 5 "TokenizerDef.rl"
	{ EmitToken(); }
	break;
	case 2:
#line 6 "TokenizerDef.rl"
	{ log("emit tx op"); StartOperatorToken(TokenOperator.Transition); }
	break;
	case 3:
#line 7 "TokenizerDef.rl"
	{ log("startKeyword"); StartToken(TokenType.Keyword); }
	break;
	case 4:
#line 8 "TokenizerDef.rl"
	{ log("startId"); StartToken(TokenType.Identifier); }
	break;
	case 5:
#line 9 "TokenizerDef.rl"
	{ log("startTransVal"); StartToken(TokenType.TransitionValue); }
	break;
#line 327 "tmp/Tokenizer.cs"
		default: break;
		}
	}

_again:
	if ( cs == 0 )
		goto _out;
	if ( ++p != pe )
		goto _resume;
	_test_eof: {}
	_out: {}
	}

#line 127 "Tokenizer.rl.cs"
         CommitLastToken();
         return _tokens;
      }

      public bool Finish()
      {
         return (cs >= Tokenizer_first_final);
      }
   }
}
